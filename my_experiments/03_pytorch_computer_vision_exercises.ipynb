{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/03_pytorch_computer_vision_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vex99np2wFVt"
   },
   "source": [
    "# 03. PyTorch Computer Vision Exercises\n",
    "\n",
    "The following is a collection of exercises based on computer vision fundamentals in PyTorch.\n",
    "\n",
    "They're a bunch of fun.\n",
    "\n",
    "You're going to get to write plenty of code!\n",
    "\n",
    "## Resources\n",
    "\n",
    "1. These exercises are based on [notebook 03 of the Learn PyTorch for Deep Learning course](https://www.learnpytorch.io/03_pytorch_computer_vision/). \n",
    "2. See a live [walkthrough of the solutions (errors and all) on YouTube](https://youtu.be/_PibmqpEyhA). \n",
    "  * **Note:** Going through these exercises took me just over 3 hours of solid coding, so you should expect around the same.\n",
    "3. See [other solutions on the course GitHub](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/extras/solutions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GaeYzOTLwWh2",
    "outputId": "17dd5453-9639-4b01-aa18-7ddbfd5c3253"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Aug  8 22:56:10 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.52.01              Driver Version: 555.99         CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...    On  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   41C    P8              2W /   55W |      92MiB /   8188MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "DNwZLMbCzJLk",
    "outputId": "9c150c50-a092-4f34-9d33-b45247fb080d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0+cu121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import torch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Import torchvision\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Import matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Exercises require PyTorch > 1.10.0\n",
    "print(torch.__version__)\n",
    "\n",
    "# TODO: Setup device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSFX7tc1w-en"
   },
   "source": [
    "## 1. What are 3 areas in industry where computer vision is currently being used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VyWRkvWGbCXj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBK-WI6YxDYa"
   },
   "source": [
    "## 2. Search \"what is overfitting in machine learning\" and write down a sentence about what you find. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1rxD6GObCqh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeYFEqw8xK26"
   },
   "source": [
    "## 3. Search \"ways to prevent overfitting in machine learning\", write down 3 of the things you find and a sentence about each. \n",
    "> **Note:** there are lots of these, so don't worry too much about all of them, just pick 3 and start with those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ocvOdWKcbEKr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKdEEFEqxM-8"
   },
   "source": [
    "## 4. Spend 20-minutes reading and clicking through the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/).\n",
    "\n",
    "* Upload your own example image using the \"upload\" button on the website and see what happens in each layer of a CNN as your image passes through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TqZaJIRMbFtS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvf-3pODxXYI"
   },
   "source": [
    "## 5. Load the [`torchvision.datasets.MNIST()`](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST) train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SHjeuN81bHza"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==1.13.0\n",
      "  Downloading torch-1.13.0-cp38-cp38-manylinux1_x86_64.whl.metadata (23 kB)\n",
      "Collecting torchvision==0.14.0\n",
      "  Using cached torchvision-0.14.0-cp38-cp38-manylinux1_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions in /home/anergcorp22/anaconda3/envs/avantleap/lib/python3.8/site-packages (from torch==1.13.0) (4.11.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/anergcorp22/anaconda3/envs/avantleap/lib/python3.8/site-packages (from torch==1.13.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/anergcorp22/anaconda3/envs/avantleap/lib/python3.8/site-packages (from torch==1.13.0) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/anergcorp22/anaconda3/envs/avantleap/lib/python3.8/site-packages (from torch==1.13.0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/anergcorp22/anaconda3/envs/avantleap/lib/python3.8/site-packages (from torch==1.13.0) (11.7.99)\n",
      "Requirement already satisfied: numpy in /home/anergcorp22/anaconda3/envs/avantleap/lib/python3.8/site-packages (from torchvision==0.14.0) (1.24.4)\n",
      "Requirement already satisfied: requests in /home/anergcorp22/anaconda3/envs/avantleap/lib/python3.8/site-packages (from torchvision==0.14.0) (2.32.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/anergcorp22/anaconda3/envs/avantleap/lib/python3.8/site-packages (from torchvision==0.14.0) (10.4.0)\n",
      "Requirement already satisfied: setuptools in /home/anergcorp22/anaconda3/envs/avantleap/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0) (71.0.2)\n",
      "Requirement already satisfied: wheel in /home/anergcorp22/anaconda3/envs/avantleap/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0) (0.43.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/anergcorp22/anaconda3/envs/avantleap/lib/python3.8/site-packages (from requests->torchvision==0.14.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/anergcorp22/anaconda3/envs/avantleap/lib/python3.8/site-packages (from requests->torchvision==0.14.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/anergcorp22/anaconda3/envs/avantleap/lib/python3.8/site-packages (from requests->torchvision==0.14.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/anergcorp22/anaconda3/envs/avantleap/lib/python3.8/site-packages (from requests->torchvision==0.14.0) (2024.7.4)\n",
      "Downloading torch-1.13.0-cp38-cp38-manylinux1_x86_64.whl (890.2 MB)\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.1/890.2 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:01:56\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch==1.13.0 torchvision==0.14.0\n",
    "\n",
    "\n",
    "# Setup training data\n",
    "train_data = datasets.FashionMNIST(\n",
    "  root=\"./data\", # where to download data to?\n",
    "  train=True, # get training data\n",
    "  download=True, # download data if it doesn't exist on disk\n",
    "  transform=ToTensor(), # images come as PIL format, we want to turn into torch tensors\n",
    "  target_transform=None # you can transform labels as well\n",
    ")\n",
    "\n",
    "# Setup test data\n",
    "test_data = datasets.FashionMNIST(\n",
    "  root=\"./data\",\n",
    "  train=False,\n",
    "  download=True,\n",
    "  transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxZW-uAbxe_F"
   },
   "source": [
    "## 6. Visualize at least 5 different samples of the MNIST training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QVFsYi1PbItE"
   },
   "outputs": [],
   "source": [
    "# See first training sample\n",
    "image, label = train_data[0]\n",
    "image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPDzW0wxhi3"
   },
   "source": [
    "## 7. Turn the MNIST train and test datasets into dataloaders using `torch.utils.data.DataLoader`, set the `batch_size=32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ALA6MPcFbJXQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCCVfXk5xjYS"
   },
   "source": [
    "## 8. Recreate `model_2` used in notebook 03 (the same model from the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/), also known as TinyVGG) capable of fitting on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5IKNF22XbKYS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sf_3zUr7xlhy"
   },
   "source": [
    "## 9. Train the model you built in exercise 8. for 5 epochs on CPU and GPU and see how long it takes on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jSo6vVWFbNLD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1CsHhPpxp1w"
   },
   "source": [
    "## 10. Make predictions using your trained model and visualize at least 5 of them comparing the prediciton to the target label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_YGgZvSobNxu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQwzqlBWxrpG"
   },
   "source": [
    "## 11. Plot a confusion matrix comparing your model's predictions to the truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vSrXiT_AbQ6e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lj6bDhoWxt2y"
   },
   "source": [
    "## 12. Create a random tensor of shape `[1, 3, 64, 64]` and pass it through a `nn.Conv2d()` layer with various hyperparameter settings (these can be any settings you choose), what do you notice if the `kernel_size` parameter goes up and down?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "leCTsqtSbR5P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHS20cNTxwSi"
   },
   "source": [
    "## 13. Use a model similar to the trained `model_2` from notebook 03 to make predictions on the test [`torchvision.datasets.FashionMNIST`](https://pytorch.org/vision/main/generated/torchvision.datasets.FashionMNIST.html) dataset. \n",
    "* Then plot some predictions where the model was wrong alongside what the label of the image should've been. \n",
    "* After visualing these predictions do you think it's more of a modelling error or a data error? \n",
    "* As in, could the model do better or are the labels of the data too close to each other (e.g. a \"Shirt\" label is too close to \"T-shirt/top\")?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78a8LjtdbSZj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMUsDcN/+FAm9Pf7Ifqs6AZ",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "03_pytorch_computer_vision_exercises.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
